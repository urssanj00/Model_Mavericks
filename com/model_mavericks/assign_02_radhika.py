# -*- coding: utf-8 -*-
"""Assign_02--radhika.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12OGFmuh8EHZUtWDGwZZXvhQqSW2zSO8M
"""

!pip install numpy pandas scikit-learn tensorflow keras xgboost

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Reshape and normalize for CNN
X_train_cnn = X_train.reshape(-1, 28, 28, 1) / 255.0
X_test_cnn = X_test.reshape(-1, 28, 28, 1) / 255.0

# Flatten for Random Forest and SVM
X_train_flatten = X_train.reshape(-1, 28*28) / 255.0
X_test_flatten = X_test.reshape(-1, 28*28) / 255.0

cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

cnn_model.fit(X_train_cnn, y_train, epochs=5, batch_size=32, validation_split=0.2)

cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)[1]
print(f"CNN Accuracy: {cnn_accuracy:.4f}")

#random forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_flatten, y_train)

#evaluate random forest
rf_predictions = rf_model.predict(X_test_flatten)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f"Random Forest Accuracy: {rf_accuracy:.4f}")

# #svm
# svm_model = SVC(kernel='rbf', probability=True, random_state=42)
# svm_model.fit(X_train_flatten, y_train)

# #evaluate svm
# svm_predictions = svm_model.predict(X_test_flatten)
# svm_accuracy = accuracy_score(y_test, svm_predictions)
# print(f"SVM Accuracy: {svm_accuracy:.4f}")

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train_flatten, y_train)

#evaluate random forest
knn_predictions = knn.predict(X_test_flatten)
knn_accuracy = accuracy_score(y_test, knn_predictions)
print(f"KNN Accuracy: {knn_accuracy:.4f}")

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

DecisionTreeClassifier_model = DecisionTreeClassifier(random_state=100)

param_grid = {
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=DecisionTreeClassifier_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')

# Flatten the train and test data
X_train_flatten = X_train.reshape(X_train.shape[0], -1)
X_test_flatten = X_test.reshape(X_test.shape[0], -1)

# Fit GridSearchCV
grid_search.fit(X_train_flatten, y_train)

# Check if GridSearchCV has found the best parameters
print(f"Best parameters: {grid_search.best_params_}")

# Get the best model and predict
best_dtree_reg = grid_search.best_estimator_
y_pred = best_dtree_reg.predict(X_test_flatten)

# Calculate RMSE
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
print(f"Test RMSE: {rmse}")

dt_gini_model = DecisionTreeClassifier(criterion="gini", random_state=100, max_depth=9, min_samples_leaf=8)
dt_gini_model.fit(X_train_flatten, y_train)

#evaluate random forest
dt_predictions = dt_gini_model.predict(X_test_flatten)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print(f"DT Accuracy: {dt_accuracy:.4f}")

dt_gini_model = DecisionTreeClassifier(criterion="entropy", random_state=100, max_depth=10, min_samples_leaf=5)
dt_gini_model.fit(X_train_flatten, y_train)

#evaluate random forest
dt_predictions = dt_gini_model.predict(X_test_flatten)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print(f"DT Accuracy: {dt_accuracy:.4f}")

ensemble_model = VotingClassifier(estimators=[
    ('knn', knn),
    ('rf', rf_model)
], voting='soft')

ensemble_model.fit(X_train_flatten, y_train)

ensemble_predictions = ensemble_model.predict(X_test_flatten)
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
print(f"Ensemble Accuracy: {ensemble_accuracy:.4f}")

#compare model performance
print(f"KNN Accuracy: {knn_accuracy:.4f}")
print(f"Random Forest Accuracy: {rf_accuracy:.4f}")
#print(f"SVM Accuracy: {svm_accuracy:.4f}")
print(f"Ensemble Accuracy: {ensemble_accuracy:.4f}")

# TO BE DECIDED
#cnn_model.save('cnn_model.h5')